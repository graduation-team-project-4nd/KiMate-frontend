// File: MultiAnalyzer.kt
package com.example.kioskassistapp.ocr

import android.graphics.PointF
import android.graphics.Rect
import android.util.Log
import androidx.annotation.OptIn
import androidx.camera.core.ExperimentalGetImage
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.pose.Pose
import com.google.mlkit.vision.pose.PoseDetection
import com.google.mlkit.vision.pose.PoseLandmark
import com.google.mlkit.vision.pose.accurate.AccuratePoseDetectorOptions
import com.google.mlkit.vision.text.TextRecognition
import com.google.mlkit.vision.text.korean.KoreanTextRecognizerOptions
import java.util.concurrent.CountDownLatch
import java.util.concurrent.Executors
import java.util.concurrent.TimeUnit

class MultiAnalyzer(
    /**
     * ëª¨ë“  ë¶„ì„(OCR, Pose)ì´ ì™„ë£Œë˜ì—ˆì„ ë•Œ í˜¸ì¶œë˜ëŠ” ë‹¨ì¼ ì½œë°±
     * @param textsAndBoxes (í…ìŠ¤íŠ¸ String, ë°•ìŠ¤ Rect) ìŒì˜ ë¦¬ìŠ¤íŠ¸
     * @param fingerTip ê°ì§€ëœ ì†ê°€ë½ ì¢Œí‘œ (ì—†ìœ¼ë©´ null)
     * @param imageWidth ë¶„ì„ì— ì‚¬ìš©ëœ ì´ë¯¸ì§€ì˜ ë„ˆë¹„
     * @param imageHeight ë¶„ì„ì— ì‚¬ìš©ëœ ì´ë¯¸ì§€ì˜ ë†’ì´
     */
    private val onAnalysisComplete: (List<Pair<String, Rect>>, PointF?, Int, Int) -> Unit
) : ImageAnalysis.Analyzer {

    private val executor = Executors.newFixedThreadPool(2)
    private val TAG = "MultiAnalyzer"

    private val ocrRecognizer = TextRecognition.getClient(KoreanTextRecognizerOptions.Builder().build())
    private val poseOptions = AccuratePoseDetectorOptions.Builder()
        .setDetectorMode(AccuratePoseDetectorOptions.STREAM_MODE)
        .build()
    private val poseDetector = PoseDetection.getClient(poseOptions)
    // â–¼â–¼â–¼ ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„ì„ ì €ì¥í•  ë³€ìˆ˜ ì¶”ê°€ â–¼â–¼â–¼
    private var lastAnalysisTime = 0L

    @OptIn(ExperimentalGetImage::class)
    override fun analyze(imageProxy: ImageProxy) {
        // â–¼â–¼â–¼ 3ì´ˆ ê°„ê²© ì²´í¬ ë¡œì§ ì¶”ê°€ â–¼â–¼â–¼
        val currentTime = System.currentTimeMillis()
        if (currentTime - lastAnalysisTime < 3000) { // 3000ms = 3ì´ˆ
            imageProxy.close() // 3ì´ˆê°€ ì•ˆ ì§€ë‚¬ìœ¼ë©´ í”„ë ˆì„ ë‹«ê³  ì¦‰ì‹œ ì¢…ë£Œ
            return
        }
        // 3ì´ˆê°€ ì§€ë‚¬ìœ¼ë©´, í˜„ì¬ ì‹œê°„ì„ ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„ìœ¼ë¡œ ê¸°ë¡
        lastAnalysisTime = currentTime
        // â–²â–²â–²
        val mediaImage = imageProxy.image
        if (mediaImage == null) {
            imageProxy.close()
            return
        }

        val rotation = imageProxy.imageInfo.rotationDegrees
        val inputImage = InputImage.fromMediaImage(mediaImage, rotation)

        // â–¼â–¼â–¼ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì½œë°±ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì €ì¥ â–¼â–¼â–¼
        val imageWidth = inputImage.width
        val imageHeight = inputImage.height
        // â–²â–²â–²

        val latch = CountDownLatch(2)

        // ê²°ê³¼ë¥¼ ë‹´ì„ ì„ì‹œ ë³€ìˆ˜
        val detectedTexts = mutableListOf<Pair<String, Rect>>()
        var fingerTip: PointF? = null

        executor.execute {
            try {
                // ğŸ”¹ OCR Task
                ocrRecognizer.process(inputImage)
                    .addOnSuccessListener { visionText ->
                        for (block in visionText.textBlocks) {
                            block.boundingBox?.let { box ->
                                // í…ìŠ¤íŠ¸ì™€ Rectë¥¼ Pairë¡œ ë¬¶ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                                detectedTexts.add(block.text to box)
                                Log.d(TAG, "Detected Text: '${block.text}' at $box")
                            }
                        }
                    }
                    .addOnFailureListener { Log.e(TAG, "OCR Failure", it) }
                    .addOnCompleteListener { latch.countDown() }

                // ğŸ”¹ Pose Task
                poseDetector.process(inputImage)
                    .addOnSuccessListener { pose: Pose ->
                        for (landmark in pose.allPoseLandmarks) {
                            if (landmark.landmarkType == PoseLandmark.RIGHT_INDEX) {
                                val confidence = landmark.inFrameLikelihood  // ğŸ‘ˆ ì´ê±° ì¶”ê°€!
                                if (confidence > 0.5f) {  // ì„ê³„ê°’ ì„¤ì •
                                    fingerTip = landmark.position
                                    Log.d(TAG, "Detected Finger Tip: (${fingerTip!!.x}, ${fingerTip!!.y}) with confidence: $confidence")
                                } else {
                                    Log.w(TAG, "Low confidence for RIGHT_INDEX: $confidence â€“ ignoring")
                                    fingerTip = null
                                }
                                break
                            }
                        }
                        if (fingerTip != null) {
                            Log.d(TAG, "Detected Finger Tip: (${fingerTip!!.x}, ${fingerTip!!.y})")
                        }
                    }
                    .addOnFailureListener { Log.e(TAG, "Pose Failure", it) }
                    .addOnCompleteListener { latch.countDown() }

            } catch (e: Exception) {
                Log.e(TAG, "Analysis Error", e)
                latch.countDown()
                latch.countDown()
            }

            // ğŸ”¹ ëª¨ë“  Task ì™„ë£Œ ëŒ€ê¸°
            try {
                if (latch.await(5, TimeUnit.SECONDS)) {
                    // â–¼â–¼â–¼ ëª¨ë“  ê²°ê³¼ë¥¼ ë‹¨ì¼ ì½œë°±ìœ¼ë¡œ ì „ë‹¬ â–¼â–¼â–¼
                    onAnalysisComplete(detectedTexts, fingerTip, imageWidth, imageHeight)
                    Log.d("OverlayDebug", "Original fingerPoint: $fingerTip (image: $$ {imageWidth}x $${imageHeight})")
                } else {
                    Log.w(TAG, "Timeout on tasks")
                }
            } catch (e: InterruptedException) {
                Log.e(TAG, "Latch await interrupted", e)
            } finally {
                imageProxy.close()
            }
        }
    }
}